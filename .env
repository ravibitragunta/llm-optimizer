# Model
MODEL_PATH=data/models/Qwen2.5-Coder-3B-Instruct-Q4_K_M.gguf
# Set n_ctx=8192 so baseline can accumulate full history for benchmarking
MODEL_N_CTX=8192
# Set to physical core count ONLY (hyperthreading hurts llama.cpp)
# Run: nproc --all to see logical, lscpu | grep 'Core(s) per socket' for physical
MODEL_N_THREADS=4
MODEL_N_BATCH=512
MODEL_USE_MLOCK=true
MODEL_USE_MMAP=true
# Lower temperature = more deterministic, better for code generation
MODEL_TEMPERATURE=0.2
MODEL_MAX_TOKENS=1024
MODEL_REPEAT_PENALTY=1.1

# Redis
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_TTL_SESSION=86400
REDIS_TTL_USER=2592000
REDIS_TTL_PATTERN=2592000

# Graph
GRAPH_MAX_NODES=500
GRAPH_MAX_EDGES=2000
GRAPH_RELEVANCE_HOPS=2
# Raised from 300 → 500 to give the LLM more active concept context
GRAPH_MAX_CONTEXT_TOKENS=500
GRAPH_DARKNESS_THRESHOLD=0.1
GRAPH_DARKNESS_DECAY=0.95
GRAPH_DARKNESS_INCREMENT=0.3
GRAPH_EDGE_INCREMENT=0.2
GRAPH_MAX_SUMMARY_RELATIONS=15

# Router
ROUTER_STATE_DIM=256
ROUTER_DECAY_RATE=0.9
ROUTER_UPDATE_RATE=0.1
ROUTER_TOP_K_CONCEPTS=5
ROUTER_CONFIDENCE_THRESHOLD=0.3

# Retriever
RETRIEVER_TOP_K_PATTERNS=3
RETRIEVER_SIMILARITY_THRESHOLD=0.7
RETRIEVER_MIN_PATTERN_FREQUENCY=2

# Prompt
# Raised from 2800 → 3500 so turns 7-10 have room for decisions + graph + prior turn
PROMPT_MAX_TOKENS=3500
PROMPT_GRAPH_TOKENS=500
PROMPT_HISTORY_TOKENS=600
PROMPT_ROUTING_TOKENS=150

# Brain
# Raised from 3 → 5 turns of history visible in prompt
BRAIN_MAX_HISTORY_TURNS=5
BRAIN_DOMAIN_DETECTION_THRESHOLD=2

# Calibration
CALIBRATION_SWEEP_DECAY_RATES=0.85,0.90,0.93,0.95,0.97
CALIBRATION_SWEEP_GRAPH_TOKENS=150,200,300,400,500
CALIBRATION_SWEEP_HISTORY_TURNS=1,2,3,4,5
CALIBRATION_SWEEP_RELEVANCE_HOPS=1,2,3
